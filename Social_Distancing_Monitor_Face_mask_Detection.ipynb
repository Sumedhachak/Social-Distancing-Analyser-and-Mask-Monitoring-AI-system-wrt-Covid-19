{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Social_Distancing_Monitor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51b913b7d3a64961be790ac873dff1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc247514e2e649ee865844ece4adf723",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dd91252575a9410c9445b3d37c6da7e7",
              "IPY_MODEL_b23eb86cad9a48b2ba71142d0d4503fe"
            ]
          }
        },
        "fc247514e2e649ee865844ece4adf723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd91252575a9410c9445b3d37c6da7e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1593832fe1c14f7aa923b36b7cfe3dc8",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481004605,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481004605,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_406f79e17b4e4fdf832e3ccc39c1b230"
          }
        },
        "b23eb86cad9a48b2ba71142d0d4503fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dc8e886189184212a3d5a784ed392d91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 459M/459M [00:30&lt;00:00, 15.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10e5ad0aa0464196b54bf9ecca6bea40"
          }
        },
        "1593832fe1c14f7aa923b36b7cfe3dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "406f79e17b4e4fdf832e3ccc39c1b230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc8e886189184212a3d5a784ed392d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10e5ad0aa0464196b54bf9ecca6bea40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6ea5c4f92b3455eb735d14cdda3a440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a83685b190d045a985f6a6838e132b2f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b30ef3fa211a4429a2e5f44d48227cb9",
              "IPY_MODEL_05b2dc2b15f54f54a1d26d28a1f2bba7"
            ]
          }
        },
        "a83685b190d045a985f6a6838e132b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b30ef3fa211a4429a2e5f44d48227cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e48c19b90da2494fb779eca268bb0d13",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 584,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9216da58c45d4d8ea9be3fe853c8acce"
          }
        },
        "05b2dc2b15f54f54a1d26d28a1f2bba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_18cb1a483af545c7a47a0a3e4e1814c3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7/584 [00:38&lt;52:55,  5.50s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04e4034e201b4acb8fee6840c5f4f280"
          }
        },
        "e48c19b90da2494fb779eca268bb0d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9216da58c45d4d8ea9be3fe853c8acce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18cb1a483af545c7a47a0a3e4e1814c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04e4034e201b4acb8fee6840c5f4f280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygq4ZQabrDEE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "55b4c8b3-811a-4b81-eca6-1a12cf95a102"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdDVjFsAYmtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f312ebed-acd5-4aa3-a875-fd74ae6c88de"
      },
      "source": [
        "%cd '/content/drive/My Drive'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvGhWHSHfU8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "d2bfa68f-8e25-4b6d-8c39-b4a69941323b"
      },
      "source": [
        "# Install Required Libraries from PyPI\n",
        "!pip install face-detection\n",
        "!pip install tqdm\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face-detection\n",
            "  Downloading https://files.pythonhosted.org/packages/63/aa/97ea9bbb2bacb1b22153ed5eb3877e52df96a03240915382c01006fd73de/face_detection-0.1.4-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from face-detection) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face-detection) (1.18.5)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face-detection) (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->face-detection) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->face-detection) (7.0.0)\n",
            "Installing collected packages: face-detection\n",
            "Successfully installed face-detection-0.1.4\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgpQJHNSfsK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Required Libraries\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import face_detection \n",
        "from keras.models import load_model\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "import tqdm\n",
        "import imutils\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aji-NiJxFy00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_threshold = 0.1\n",
        "nms_threshold = 0.1\n",
        "angle_factor = 0.4\n",
        "H_zoom_factor = 1.2"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE1la1fzF72l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dist(c1, c2):\n",
        "    return ((c1[0] - c2[0]) ** 2 + (c1[1] - c2[1]) ** 2) ** 0.5\n",
        "\n",
        "def T2S(T):\n",
        "    S = abs(T/((1+T**2)**0.5))\n",
        "    return S\n",
        "\n",
        "def T2C(T):\n",
        "    C = abs(1/((1+T**2)**0.5))\n",
        "    return C\n",
        "\n",
        "def isclose(p1,p2):\n",
        "\n",
        "    c_d = dist(p1[2], p2[2])\n",
        "    if(p1[1]<p2[1]):\n",
        "        a_w = p1[0]\n",
        "        a_h = p1[1]\n",
        "    else:\n",
        "        a_w = p2[0]\n",
        "        a_h = p2[1]\n",
        "\n",
        "    T = 0\n",
        "    try:\n",
        "        T=(p2[2][1]-p1[2][1])/(p2[2][0]-p1[2][0])\n",
        "    except ZeroDivisionError:\n",
        "        T = 1.633123935319537e+16\n",
        "    S = T2S(T)\n",
        "    C = T2C(T)\n",
        "    d_hor = C*c_d\n",
        "    d_ver = S*c_d\n",
        "    vc_calib_hor = a_w*1.3\n",
        "    vc_calib_ver = a_h*0.4*angle_factor\n",
        "    c_calib_hor = a_w *1.7\n",
        "    c_calib_ver = a_h*0.2*angle_factor\n",
        "    # print(p1[2], p2[2],(vc_calib_hor,d_hor),(vc_calib_ver,d_ver))\n",
        "    if (0<d_hor<vc_calib_hor and 0<d_ver<vc_calib_ver):\n",
        "        return 1\n",
        "    elif 0<d_hor<c_calib_hor and 0<d_ver<c_calib_ver:\n",
        "        return 2\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygki7lR_sFvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path to the Working Environment\n",
        "\n",
        "# If using Google Colab (If on a Local Environment, no path required => set BASE_PATH  = \"\")\n",
        "BASE_PATH = \"Social_Distancing_with_AI/\"\n",
        "\n",
        "# Path to Input Video File in the BASE_PATH\n",
        "FILE_PATH = \"test_video.mp4\""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xrRSzoT9EBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "51b913b7d3a64961be790ac873dff1c2",
            "fc247514e2e649ee865844ece4adf723",
            "dd91252575a9410c9445b3d37c6da7e7",
            "b23eb86cad9a48b2ba71142d0d4503fe",
            "1593832fe1c14f7aa923b36b7cfe3dc8",
            "406f79e17b4e4fdf832e3ccc39c1b230",
            "dc8e886189184212a3d5a784ed392d91",
            "10e5ad0aa0464196b54bf9ecca6bea40"
          ]
        },
        "outputId": "27d568ee-a88f-4dd6-a5eb-37bc6587af62"
      },
      "source": [
        "# Initialize a Face Detector \n",
        "# Confidence Threshold can be Adjusted, Greater values would Detect only Clear Faces\n",
        "detector = face_detection.build_detector(\"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"http://folk.ntnu.no//haakohu/WIDERFace_DSFD_RES152.pth\" to /root/.cache/torch/checkpoints/WIDERFace_DSFD_RES152.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51b913b7d3a64961be790ac873dff1c2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=481004605.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv-5woacC0C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Pretrained Face Mask Classfier (Keras Model)\n",
        "mask_classifier = load_model(\"Social-Distancing-Analyser-and-Mask-Monitoring-AI-system-wrt-Covid-19/Models/ResNet50_Classifier.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5a7o3HHFVv2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "c6ea5c4f92b3455eb735d14cdda3a440",
            "a83685b190d045a985f6a6838e132b2f",
            "b30ef3fa211a4429a2e5f44d48227cb9",
            "05b2dc2b15f54f54a1d26d28a1f2bba7",
            "e48c19b90da2494fb779eca268bb0d13",
            "9216da58c45d4d8ea9be3fe853c8acce",
            "18cb1a483af545c7a47a0a3e4e1814c3",
            "04e4034e201b4acb8fee6840c5f4f280"
          ]
        },
        "outputId": "d3615c1c-c869-45b9-8281-f77da3fe8bb3"
      },
      "source": [
        "##################################### Analyze the Video ################################################\n",
        "\n",
        "# Load YOLOv3\n",
        "net = cv2.dnn.readNet(BASE_PATH+\"Models/\"+\"yolov3-spp.weights\", BASE_PATH+\"Models/\"+\"yolov3-spp.cfg\")\n",
        "\n",
        "# Load COCO Classes\n",
        "LABELS  = []\n",
        "with open(BASE_PATH+\"Models/\"+\"coco.names\", \"r\") as f:\n",
        "    LABELS = [line.strip() for line in f.readlines()]\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "FR=0\n",
        "# Fetch Video Properties\n",
        "vs = cv2.VideoCapture(BASE_PATH + FILE_PATH )\n",
        "fps = vs.get(cv2.CAP_PROP_FPS)\n",
        "print(fps)\n",
        "# width1 = vs.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "# height1 = vs.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "n_frames = vs.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "print(n_frames)\n",
        "writer = None\n",
        "(W, H) = (None, None)\n",
        "\n",
        "fl = 0\n",
        "q = 0\n",
        "# Create Directory for Storing Results (Make sure it doesn't already exists !)\n",
        "\n",
        "# Initialize Output Video Stream\n",
        "# out_stream = cv2.VideoWriter(\n",
        "#     BASE_PATH+'Results/test.mp4',\n",
        "#     cv2.VideoWriter_fourcc('X','V','I','D'),\n",
        "#     fps,\n",
        "#     (int(width),int(height)))\n",
        "\n",
        "print(\"Processing Frames :\")\n",
        "for feed in tqdm.notebook.tqdm(range(int(n_frames))):\n",
        "  \n",
        "    # Capture Frame-by-Frame\n",
        "    (grabbed, frame) = vs.read()\n",
        "    frame = imutils.resize(frame, width=1280,height=720)\n",
        "    # Check EOF\n",
        "    if not grabbed:\n",
        "        break\n",
        "\n",
        "    if W is None or H is None:\n",
        "        (H, W) = frame.shape[:2]\n",
        "        FW=W\n",
        "        # if(W<1075):\n",
        "        #     FW = 1075\n",
        "        FR = np.zeros((H+210,FW,3), np.uint8)\n",
        "\n",
        "        col = (255,255,255)\n",
        "        FH = H + 210\n",
        "    FR[:] = col\n",
        "\n",
        "    # Detect Objects in the Frame with YOLOv3\n",
        "    # blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (608, 608), swapRB=True, crop=False)\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0 , (608, 608), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    start = time.time()\n",
        "    outs = net.forward(output_layers)\n",
        "    end = time.time()\n",
        "\n",
        "    classIDs  = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    \n",
        "    # Store Detected Objects with Labels, Bounding_Boxes and their Confidences\n",
        "    for out in outs:\n",
        "\n",
        "        for detection in out:\n",
        "\n",
        "\n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            confidence = scores[classID]\n",
        "            if LABELS[classID] == \"person\":\n",
        "                if confidence > conf_threshold:\n",
        "                \n",
        "                    # Get Center, Height and Width of the Box\n",
        "                    box = detection[0:4] * np.array([W, H, W, H])\n",
        "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "                    x = int(centerX - (width / 2))\n",
        "                    y = int(centerY - (height / 2))\n",
        "                    \n",
        "                \n",
        "                    # Topleft Co-ordinates\n",
        "                    # x = int(center_x - w / 2)\n",
        "                    # y = int(center_y - h / 2)\n",
        "\n",
        "                    boxes.append([x, y,int(width), int(height)])\n",
        "                    confidences.append(float(confidence))\n",
        "                    classIDs.append(classID)\n",
        "\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold,nms_threshold)\n",
        "\n",
        "    # Initialize empty lists for storing Bounding Boxes of People and their Faces\n",
        "    \n",
        "\n",
        "    # Work on Detected Persons in the Frame\n",
        "    if len(idxs) > 0:\n",
        "        persons = []\n",
        "        masked_faces = []\n",
        "        unmasked_faces = []\n",
        "        status = []\n",
        "        idf = idxs.flatten()\n",
        "        close_pair = []\n",
        "        s_close_pair = []\n",
        "        center = []\n",
        "        co_info = []\n",
        "\n",
        "        for i in idf:\n",
        "\n",
        "            # (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            # (w, h) = (boxes[i][2], boxes[i][3])\n",
        "            box = np.array(boxes[i])\n",
        "            box = np.where(box<0,0,box)\n",
        "            (x, y, w, h) = box\n",
        "            cen = [int(x + w / 2), int(y + h / 2)]\n",
        "            center.append(cen)\n",
        "            cv2.circle(frame, tuple(cen),1,(0,0,0),1)\n",
        "            co_info.append([w, h, cen])\n",
        "\n",
        "            status.append(0)\n",
        "            persons.append([x,y,w,h])\n",
        "                \n",
        "                # Detect Face in the Person\n",
        "            person_rgb = frame[y:y+h,x:x+w,::-1]   # Crop & BGR to RGB\n",
        "            detections = detector.detect(person_rgb)\n",
        "\n",
        "                # If a Face is Detected\n",
        "            if detections.shape[0] > 0:\n",
        "\n",
        "                detection = np.array(detections[0])\n",
        "                detection = np.where(detection<0,0,detection)\n",
        "\n",
        "                # Calculating Co-ordinates of the Detected Face\n",
        "                x1 = x + int(detection[0])\n",
        "                x2 = x + int(detection[2])\n",
        "                y1 = y + int(detection[1])\n",
        "                y2 = y + int(detection[3])\n",
        "\n",
        "                try :\n",
        "\n",
        "\n",
        "                    # Crop & BGR to RGB\n",
        "                    face_rgb = frame[y1:y2,x1:x2,::-1]  \n",
        "\n",
        "                    # Preprocess the Image\n",
        "                    face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "                    face_arr = np.expand_dims(face_arr, axis=0)\n",
        "                    face_arr = preprocess_input(face_arr)\n",
        "\n",
        "                    # Predict if the Face is Masked or Not\n",
        "                    score = mask_classifier.predict(face_arr)\n",
        "\n",
        "                    # Determine and store Results\n",
        "                    if score[0][0]<0.10:\n",
        "                        masked_faces.append([x1,y1,x2,y2])\n",
        "                    else:\n",
        "                        unmasked_faces.append([x1,y1,x2,y2])\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "        masked_face_count = len(masked_faces)\n",
        "        unmasked_face_count = len(unmasked_faces)\n",
        "        for i in range(len(center)):\n",
        "            for j in range(len(center)):\n",
        "    \n",
        "                g = isclose(co_info[i],co_info[j])\n",
        "\n",
        "                if g == 1:\n",
        "\n",
        "                    close_pair.append([center[i], center[j]])\n",
        "                    status[i] = 1\n",
        "                    status[j] = 1\n",
        "                elif g == 2:\n",
        "                    s_close_pair.append([center[i], center[j]])\n",
        "                    if status[i] != 1:\n",
        "                        status[i] = 2\n",
        "                    if status[j] != 1:\n",
        "                        status[j] = 2\n",
        "        total_p = len(center)\n",
        "        low_risk_p = status.count(2)\n",
        "        high_risk_p = status.count(1)\n",
        "        safe_p = status.count(0)\n",
        "        kk = 0                       \n",
        "        for i in idf:\n",
        "            cv2.line(FR,(0,H+1),(FW,H+1),(0,0,0),2)\n",
        "            cv2.putText(FR, \"Social Distancing Analyser and Mask Monitoring wrt. COVID-19\", (210, H+60),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
        "            cv2.rectangle(FR, (20, H+80), (510, H+180), (100, 100, 100), 2)\n",
        "            cv2.putText(FR, \"Connecting lines shows closeness among people. \", (30, H+100),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 0), 2)\n",
        "            cv2.putText(FR, \"-- YELLOW: CLOSE\", (50, H+90+40),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 170, 170), 2)\n",
        "            cv2.putText(FR, \"--    RED: VERY CLOSE\", (50, H+40+110),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "            # cv2.putText(frame, \"--    PINK: Pathway for Calibration\", (50, 150),\n",
        "            #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180,105,255), 1)\n",
        "\n",
        "            cv2.rectangle(FR, (535, H+80), (1250, H+140+40), (100, 100, 100), 2)\n",
        "            cv2.putText(FR, \"Bounding box shows the level of risk to the person and Mask Monitoring\", (545, H+100),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 0), 2)\n",
        "            cv2.putText(FR, \"-- LIGHT GREEN: SAFE\", (565,  H+90+40),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            cv2.putText(FR, \"-- Green: MASKED\", (865, H+90+40),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 100, 0), 2)\n",
        "            cv2.putText(FR, \"--    DARK RED: HIGH RISK\", (565, H+150),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 150), 2)\n",
        "            cv2.putText(FR, \"--   RED: UNMASKED\", (865, H+150),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "            cv2.putText(FR, \"--      ORANGE: LOW RISK\", (565, H+170),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 120, 255), 2)\n",
        "            \n",
        "            tot_str = \"TOTAL COUNT:\" + str(total_p)\n",
        "            high_str = \"HIGH RISK COUNT:\" + str(high_risk_p)\n",
        "            low_str = \"LOW RISK COUNT:\" + str(low_risk_p)\n",
        "            safe_str = \"SAFE COUNT:\" + str(safe_p)\n",
        "            masked_str=\"MASKED COUNT:\" + str(masked_face_count)\n",
        "            unmasked_str=\"UNMASKED COUNT:\" + str(unmasked_face_count)\n",
        "            unknown_str=\"UNKNOWN COUNT:\" + str(total_p-masked_face_count-unmasked_face_count)\n",
        "            cv2.putText(FR, tot_str, (1, H +25),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
        "            cv2.putText(FR, safe_str, (160, H +25),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "            cv2.putText(FR, low_str, (310, H +25),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 120, 255), 2)\n",
        "            cv2.putText(FR, high_str, (500, H +25),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 150), 2)\n",
        "            cv2.putText(FR, masked_str, (700, H +25),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 100, 0), 2)\n",
        "            cv2.putText(FR, unmasked_str, (880, H +25),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "            cv2.putText(FR, unknown_str, (1080, H +25),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
        "            (x, y) = (boxes[i][0], boxes[i][1])\n",
        "            (w, h) = (boxes[i][2], boxes[i][3])\n",
        "            if status[kk] == 1:\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 150), 2)\n",
        "              # Save Image of Cropped Person violating social distance (If not required, comment the command below)\n",
        "              # cv2.imwrite(BASE_PATH + \"Results/Social_Distance_Violators/\"+str(feed)+\"_\"+str(len(persons))+\".jpg\",frame[x:y,x+w:y+h])\n",
        "\n",
        "            elif status[kk] == 0:\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "            else:\n",
        "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 120, 255), 2)\n",
        "\n",
        "            kk += 1\n",
        "        for h in close_pair:\n",
        "            cv2.line(frame, tuple(h[0]), tuple(h[1]), (0, 0, 255), 2)\n",
        "        for b in s_close_pair:\n",
        "            cv2.line(frame, tuple(b[0]), tuple(b[1]), (0, 255, 255), 2)            \n",
        "\n",
        "        # Put Bounding Boxes on Faces in the Frame\n",
        "        # Green if Safe, Red if UnSafe\n",
        "        for f in range(masked_face_count):\n",
        "            a,b,c,d = masked_faces[f]\n",
        "            cv2.rectangle(frame, (a, b), (c,d), (0,100,0), 2)\n",
        "\n",
        "        for f in range(unmasked_face_count):\n",
        "            a,b,c,d = unmasked_faces[f]\n",
        "            cv2.rectangle(frame, (a, b), (c,d), (0,0,255), 2)\n",
        "\n",
        "        FR[0:H, 0:W] = frame  \n",
        "        frame = FR\n",
        "    # Write Frame to the Output File\n",
        "    \n",
        "    # Save the Frame in frame_no.png format (If not required, comment the command below)\n",
        "    # cv2.imwrite(BASE_PATH+\"Results1/Frames/\"+str(frame)+\".jpg\",img)\n",
        "\n",
        "    # Use if you want to see Results Frame by Frame\n",
        "        # cv2_imshow(frame)\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "    # Exit on Pressing Q Key\n",
        "    # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "    #     break\n",
        "    if writer is None:\n",
        "        fourcc = cv2.VideoWriter_fourcc('X','V','I','D')\n",
        "        writer = cv2.VideoWriter(BASE_PATH+'Results/test.mp4', fourcc, 30,(frame.shape[1], frame.shape[0]), True)\n",
        "    writer.write(frame)\n",
        "\n",
        "# Release Streams\n",
        "writer.release()\n",
        "vs.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Good to Go!\n",
        "print(\"Done !\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23.97\n",
            "584.0\n",
            "Processing Frames :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6ea5c4f92b3455eb735d14cdda3a440",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=584.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZAYgaUVOo24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
