{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Social_Distancing_Monitor.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cd7717675f7f46c7b0f389f2cb1f0505":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_244ad84e83f94925b47a2168335b5435","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_209d9cbd633e404986e2f7e4571facce","IPY_MODEL_1b5cb877971f45ba8a8996e594806fbf"]}},"244ad84e83f94925b47a2168335b5435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"209d9cbd633e404986e2f7e4571facce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bb6b0e51b4c34624b9ef59af7eecf517","_dom_classes":[],"description":"  1%","_model_name":"FloatProgressModel","bar_style":"","max":584,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_65c7ad8fc62b4a72bf7adfaf340b6c44"}},"1b5cb877971f45ba8a8996e594806fbf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6f33f0cb90e84ce3a27cd11c795b111a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/584 [00:17&lt;46:02,  4.76s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b05d466991ad43b295e2c2d3c4b6fbbf"}},"bb6b0e51b4c34624b9ef59af7eecf517":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"65c7ad8fc62b4a72bf7adfaf340b6c44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f33f0cb90e84ce3a27cd11c795b111a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b05d466991ad43b295e2c2d3c4b6fbbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"Ygq4ZQabrDEE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595449273544,"user_tz":-330,"elapsed":2035,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"baabf4f5-0431-4cc4-9213-d3bbafbc2729"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZdDVjFsAYmtp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595449278220,"user_tz":-330,"elapsed":1664,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"024a9c11-9fcf-48ea-a58f-dfa6421cf571"},"source":["%cd '/content/drive/My Drive'"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tvGhWHSHfU8B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1595449293005,"user_tz":-330,"elapsed":12974,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"7101ba96-ff8f-4871-e613-3dffa9b5707d"},"source":["# Install Required Libraries from PyPI\n","!pip install face-detection\n","!pip install tqdm"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: face-detection in /usr/local/lib/python3.6/dist-packages (0.1.4)\n","Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face-detection) (0.6.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face-detection) (1.18.5)\n","Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from face-detection) (1.5.1+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->face-detection) (7.0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->face-detection) (0.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KgpQJHNSfsK6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595449306872,"user_tz":-330,"elapsed":4641,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}},"outputId":"fcd9997a-41e5-4a6f-97a2-a3a7eb6c528a"},"source":["# Import Required Libraries\n","import time\n","import math\n","import os\n","import numpy as np\n","import cv2\n","import face_detection \n","from keras.models import load_model\n","from keras.applications.resnet50 import preprocess_input\n","import tqdm\n","from google.colab.patches import cv2_imshow"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aji-NiJxFy00","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595449316953,"user_tz":-330,"elapsed":1669,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}}},"source":["confid = 0.5\n","thresh = 0.5\n","angle_factor = 0.9\n","H_zoom_factor = 1.2"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"GE1la1fzF72l","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595449322597,"user_tz":-330,"elapsed":2271,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}}},"source":["def dist(c1, c2):\n","    return ((c1[0] - c2[0]) ** 2 + (c1[1] - c2[1]) ** 2) ** 0.5\n","\n","def T2S(T):\n","    S = abs(T/((1+T**2)**0.5))\n","    return S\n","\n","def T2C(T):\n","    C = abs(1/((1+T**2)**0.5))\n","    return C\n","\n","def isclose(p1,p2):\n","\n","    c_d = dist(p1[2], p2[2])\n","    if(p1[1]<p2[1]):\n","        a_w = p1[0]\n","        a_h = p1[1]\n","    else:\n","        a_w = p2[0]\n","        a_h = p2[1]\n","\n","    T = 0\n","    try:\n","        T=(p2[2][1]-p1[2][1])/(p2[2][0]-p1[2][0])\n","    except ZeroDivisionError:\n","        T = 1.633123935319537e+16\n","    S = T2S(T)\n","    C = T2C(T)\n","    d_hor = C*c_d\n","    d_ver = S*c_d\n","    vc_calib_hor = a_w*1.3\n","    vc_calib_ver = a_h*0.4*angle_factor\n","    c_calib_hor = a_w *1.7\n","    c_calib_ver = a_h*0.2*angle_factor\n","    # print(p1[2], p2[2],(vc_calib_hor,d_hor),(vc_calib_ver,d_ver))\n","    if (0<d_hor<vc_calib_hor and 0<d_ver<vc_calib_ver):\n","        return 1\n","    elif 0<d_hor<c_calib_hor and 0<d_ver<c_calib_ver:\n","        return 2\n","    else:\n","        return 0"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"ygki7lR_sFvf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595449330848,"user_tz":-330,"elapsed":1214,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}}},"source":["# Path to the Working Environment\n","\n","# If using Google Colab (If on a Local Environment, no path required => set BASE_PATH  = \"\")\n","BASE_PATH = \"Social_Distancing_with_AI/\"\n","\n","# Path to Input Video File in the BASE_PATH\n","FILE_PATH = \"test_video.mp4\""],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xrRSzoT9EBa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595449351917,"user_tz":-330,"elapsed":6474,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}}},"source":["# Initialize a Face Detector \n","# Confidence Threshold can be Adjusted, Greater values would Detect only Clear Faces\n","detector = face_detection.build_detector(\"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.4)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kv-5woacC0C5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595449381610,"user_tz":-330,"elapsed":24242,"user":{"displayName":"sk walia","photoUrl":"","userId":"14137336038249647208"}}},"source":["# Load Pretrained Face Mask Classfier (Keras Model)\n","mask_classifier = load_model(\"Social_Distancing_with_AI/Models/ResNet50_Classifier.h5\")"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5a7o3HHFVv2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cd7717675f7f46c7b0f389f2cb1f0505","244ad84e83f94925b47a2168335b5435","209d9cbd633e404986e2f7e4571facce","1b5cb877971f45ba8a8996e594806fbf","bb6b0e51b4c34624b9ef59af7eecf517","65c7ad8fc62b4a72bf7adfaf340b6c44","6f33f0cb90e84ce3a27cd11c795b111a","b05d466991ad43b295e2c2d3c4b6fbbf"],"output_embedded_package_id":"1pDyBjH9oEQmtiXQ15-HiZcKrPu8NMgXc"},"outputId":"476c4259-1bc8-45f6-cd0e-eb8ed4661486"},"source":["##################################### Analyze the Video ################################################\n","\n","# Load YOLOv3\n","net = cv2.dnn.readNet(BASE_PATH+\"Models/\"+\"yolov3.weights\", BASE_PATH+\"Models/\"+\"yolov3.cfg\")\n","\n","# Load COCO Classes\n","LABELS  = []\n","with open(BASE_PATH+\"Models/\"+\"coco.names\", \"r\") as f:\n","    LABELS = [line.strip() for line in f.readlines()]\n","\n","np.random.seed(42)\n","\n","layer_names = net.getLayerNames()\n","output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","FR=0\n","# Fetch Video Properties\n","vs = cv2.VideoCapture(BASE_PATH + FILE_PATH )\n","fps = vs.get(cv2.CAP_PROP_FPS)\n","print(fps)\n","width1 = vs.get(cv2.CAP_PROP_FRAME_WIDTH)\n","height1 = vs.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","n_frames = vs.get(cv2.CAP_PROP_FRAME_COUNT)\n","print(n_frames)\n","writer = None\n","(W, H) = (None, None)\n","\n","fl = 0\n","q = 0\n","# Create Directory for Storing Results (Make sure it doesn't already exists !)\n","\n","# Initialize Output Video Stream\n","# out_stream = cv2.VideoWriter(\n","#     BASE_PATH+'Results/test.mp4',\n","#     cv2.VideoWriter_fourcc('X','V','I','D'),\n","#     fps,\n","#     (int(width),int(height)))\n","\n","print(\"Processing Frames :\")\n","for feed in tqdm.notebook.tqdm(range(int(n_frames))):\n","  \n","    # Capture Frame-by-Frame\n","    (grabbed, frame) = vs.read()\n","\n","    # Check EOF\n","    if not grabbed:\n","        break\n","\n","    if W is None or H is None:\n","        (H, W) = frame.shape[:2]\n","        FW=W\n","        if(W<1075):\n","            FW = 1075\n","        FR = np.zeros((H+210,FW,3), np.uint8)\n","\n","        col = (255,255,255)\n","        FH = H + 210\n","    FR[:] = col\n","\n","    # Detect Objects in the Frame with YOLOv3\n","    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n","    net.setInput(blob)\n","    start = time.time()\n","    outs = net.forward(output_layers)\n","    end = time.time()\n","\n","    classIDs  = []\n","    confidences = []\n","    boxes = []\n","    \n","    # Store Detected Objects with Labels, Bounding_Boxes and their Confidences\n","    for out in outs:\n","\n","        for detection in out:\n","\n","\n","            scores = detection[5:]\n","            classID = np.argmax(scores)\n","            confidence = scores[classID]\n","            if LABELS[classID] == \"person\":\n","                if confidence > confid:\n","                \n","                    # Get Center, Height and Width of the Box\n","                    box = detection[0:4] * np.array([W, H, W, H])\n","                    (centerX, centerY, width, height) = box.astype(\"int\")\n","                    x = int(centerX - (width / 2))\n","                    y = int(centerY - (height / 2))\n","                    \n","                \n","                    # Topleft Co-ordinates\n","                    # x = int(center_x - w / 2)\n","                    # y = int(center_y - h / 2)\n","\n","                    boxes.append([x, y,int(width), int(height)])\n","                    confidences.append(float(confidence))\n","                    classIDs.append(classID)\n","\n","    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confid,thresh)\n","\n","    # Initialize empty lists for storing Bounding Boxes of People and their Faces\n","    \n","\n","    # Work on Detected Persons in the Frame\n","    if len(idxs) > 0:\n","        persons = []\n","        masked_faces = []\n","        unmasked_faces = []\n","        status = []\n","        idf = idxs.flatten()\n","        close_pair = []\n","        s_close_pair = []\n","        center = []\n","        co_info = []\n","\n","        for i in idf:\n","\n","            (x, y) = (boxes[i][0], boxes[i][1])\n","            (w, h) = (boxes[i][2], boxes[i][3])\n","    \n","            cen = [int(x + w / 2), int(y + h / 2)]\n","            center.append(cen)\n","            cv2.circle(frame, tuple(cen),1,(0,0,0),1)\n","            co_info.append([w, h, cen])\n","\n","            status.append(0)\n","            persons.append([x,y,w,h])\n","                \n","              \n","\n","                # Detect Face in the Person\n","            person_rgb = frame[y:y+h,x:x+w,::-1]   # Crop & BGR to RGB\n","            detections = detector.detect(person_rgb)\n","\n","                # If a Face is Detected\n","            if detections.shape[0] > 0:\n","\n","                detection = np.array(detections[0])\n","                detection = np.where(detection<0,0,detection)\n","\n","                # Calculating Co-ordinates of the Detected Face\n","                x1 = x + int(detection[0])\n","                x2 = x + int(detection[2])\n","                y1 = y + int(detection[1])\n","                y2 = y + int(detection[3])\n","\n","                try :\n","\n","\n","                    # Crop & BGR to RGB\n","                    face_rgb = frame[y1:y2,x1:x2,::-1]   \n","\n","                    # Preprocess the Image\n","                    face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n","                    face_arr = np.expand_dims(face_arr, axis=0)\n","                    face_arr = preprocess_input(face_arr)\n","\n","                    # Predict if the Face is Masked or Not\n","                    score = mask_classifier.predict(face_arr)\n","\n","                    # Determine and store Results\n","                    if score[0][0]<0.10:\n","                        masked_faces.append([x1,y1,x2,y2])\n","                    else:\n","                        unmasked_faces.append([x1,y1,x2,y2])\n","\n","                except:\n","                    continue\n","        masked_face_count = len(masked_faces)\n","        unmasked_face_count = len(unmasked_faces)\n","        for i in range(len(center)):\n","            for j in range(len(center)):\n","    \n","                g = isclose(co_info[i],co_info[j])\n","\n","                if g == 1:\n","\n","                    close_pair.append([center[i], center[j]])\n","                    status[i] = 1\n","                    status[j] = 1\n","                elif g == 2:\n","                    s_close_pair.append([center[i], center[j]])\n","                    if status[i] != 1:\n","                        status[i] = 2\n","                    if status[j] != 1:\n","                        status[j] = 2\n","        total_p = len(center)\n","        low_risk_p = status.count(2)\n","        high_risk_p = status.count(1)\n","        safe_p = status.count(0)\n","        kk = 0                       \n","        for i in idf:\n","            cv2.line(FR,(0,H+1),(FW,H+1),(0,0,0),2)\n","            cv2.putText(FR, \"Social Distancing Analyser and Mask Monitoring wrt. COVID-19\", (210, H+60),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n","            cv2.rectangle(FR, (20, H+80), (510, H+180), (100, 100, 100), 2)\n","            cv2.putText(FR, \"Connecting lines shows closeness among people. \", (30, H+100),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 0), 2)\n","            cv2.putText(FR, \"-- YELLOW: CLOSE\", (50, H+90+40),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 170, 170), 2)\n","            cv2.putText(FR, \"--    RED: VERY CLOSE\", (50, H+40+110),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n","            # cv2.putText(frame, \"--    PINK: Pathway for Calibration\", (50, 150),\n","            #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180,105,255), 1)\n","\n","            cv2.rectangle(FR, (535, H+80), (1250, H+140+40), (100, 100, 100), 2)\n","            cv2.putText(FR, \"Bounding box shows the level of risk to the person and Mask Monitoring\", (545, H+100),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 0), 2)\n","            cv2.putText(FR, \"-- LIGHT GREEN: SAFE\", (565,  H+90+40),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","            cv2.putText(FR, \"-- Green: MASKED\", (865, H+90+40),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 100, 0), 2)\n","            cv2.putText(FR, \"--    DARK RED: HIGH RISK\", (565, H+150),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 150), 2)\n","            cv2.putText(FR, \"--   RED: UNMASKED\", (865, H+150),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n","            cv2.putText(FR, \"--      ORANGE: LOW RISK\", (565, H+170),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 120, 255), 2)\n","            \n","            tot_str = \"TOTAL COUNT:\" + str(total_p)\n","            high_str = \"HIGH RISK COUNT:\" + str(high_risk_p)\n","            low_str = \"LOW RISK COUNT:\" + str(low_risk_p)\n","            safe_str = \"SAFE COUNT:\" + str(safe_p)\n","            masked_str=\"MASKED COUNT:\" + str(masked_face_count)\n","            unmasked_str=\"UNMASKED COUNT:\" + str(unmasked_face_count)\n","            unknown_str=\"UNKNOWN COUNT:\" + str(total_p-masked_face_count-unmasked_face_count)\n","            cv2.putText(FR, tot_str, (1, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n","            cv2.putText(FR, safe_str, (160, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n","            cv2.putText(FR, low_str, (310, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 120, 255), 2)\n","            cv2.putText(FR, high_str, (500, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 150), 2)\n","            cv2.putText(FR, masked_str, (700, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 100, 0), 2)\n","            cv2.putText(FR, unmasked_str, (880, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n","            cv2.putText(FR, unknown_str, (1080, H +25),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n","            (x, y) = (boxes[i][0], boxes[i][1])\n","            (w, h) = (boxes[i][2], boxes[i][3])\n","            if status[kk] == 1:\n","                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 150), 2)\n","              # Save Image of Cropped Person violating social distance (If not required, comment the command below)\n","              # cv2.imwrite(BASE_PATH + \"Results/Social_Distance_Violators/\"+str(feed)+\"_\"+str(len(persons))+\".jpg\",frame[x:y,x+w:y+h])\n","\n","            elif status[kk] == 0:\n","                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","\n","            else:\n","                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 120, 255), 2)\n","\n","            kk += 1\n","        for h in close_pair:\n","            cv2.line(frame, tuple(h[0]), tuple(h[1]), (0, 0, 255), 2)\n","        for b in s_close_pair:\n","            cv2.line(frame, tuple(b[0]), tuple(b[1]), (0, 255, 255), 2)            \n","\n","        # Put Bounding Boxes on Faces in the Frame\n","        # Green if Safe, Red if UnSafe\n","        for f in range(masked_face_count):\n","            a,b,c,d = masked_faces[f]\n","            cv2.rectangle(frame, (a, b), (c,d), (0,100,0), 2)\n","\n","        for f in range(unmasked_face_count):\n","            a,b,c,d = unmasked_faces[f]\n","            cv2.rectangle(frame, (a, b), (c,d), (0,0,255), 2)\n","\n","        FR[0:H, 0:W] = frame  \n","        frame = FR\n","    # Write Frame to the Output File\n","    \n","    # Save the Frame in frame_no.png format (If not required, comment the command below)\n","    # cv2.imwrite(BASE_PATH+\"Results1/Frames/\"+str(frame)+\".jpg\",img)\n","\n","    # Use if you want to see Results Frame by Frame\n","        cv2_imshow(frame)\n","        cv2.waitKey(1)\n","\n","    # Exit on Pressing Q Key\n","    # if cv2.waitKey(25) & 0xFF == ord('q'):\n","    #     break\n","    if writer is None:\n","        fourcc = cv2.VideoWriter_fourcc('X','V','I','D')\n","        writer = cv2.VideoWriter(BASE_PATH+'Results/test1.mp4', fourcc, 30,(frame.shape[1], frame.shape[0]), True)\n","    writer.write(frame)\n","\n","\n","# Release Streams\n","writer.release()\n","vs.release()\n","cv2.destroyAllWindows()\n","\n","# Good to Go!\n","print(\"Done !\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"JS_Bjm8uMpe-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}
